{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 00:23:42.899516: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-18 00:23:43.085545: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-18 00:23:43.857064: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/emre/anaconda3/envs/emre_venv/lib/python3.9/site-packages/cv2/../../lib64::/home/emre/anaconda3/envs/emre_venv/lib/:/home/emre/anaconda3/envs/emre_venv/lib/\n",
      "2022-10-18 00:23:43.857254: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/emre/anaconda3/envs/emre_venv/lib/python3.9/site-packages/cv2/../../lib64::/home/emre/anaconda3/envs/emre_venv/lib/:/home/emre/anaconda3/envs/emre_venv/lib/\n",
      "2022-10-18 00:23:43.857272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File to be created: /mnt/HDD04/ICASSP_2023/Scripts/datasets/imit_ASL_10_vid_skel_224x224rgb_down.pkl\n"
     ]
    }
   ],
   "source": [
    "import glob, h5py, pickle, cv2, re\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mat73\n",
    "from IPython.display import clear_output\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "width = 224\n",
    "height = 224\n",
    "channels = 3\n",
    "\n",
    "num_class = 10\n",
    "sub = 'imit_ASL_' + str(num_class) + '_vid_skel_224x224rgb_down'\n",
    "filename = '/mnt/HDD04/ICASSP_2023/Scripts/datasets/'+sub+'.pkl'\n",
    "print('File to be created: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. env files: 0\n",
      "Num. files: 1980\n"
     ]
    }
   ],
   "source": [
    "env_path = '/mnt/HDD04/ASL_10_imit/outputs/envelopes_sim/'\n",
    "env_threshold = 2e3\n",
    "err_path = '/mnt/HDD04/ASL_10_imit/scripts/err_files.txt'\n",
    "vid_path = '/mnt/HDD04/ASL_10_imit/outputs/webcam/cut/'\n",
    "skel_path = env_path.replace('envelopes_sim', 'skeletons_mat')\n",
    "md_path = vid_path.replace('webcam', 'microDoppler_real')\n",
    "\n",
    "md_files = glob.glob(md_path + '*png')\n",
    "env_files = glob.glob(env_path + '*txt')\n",
    "fnames = [m.split('/')[-1][:-4] for m in md_files]\n",
    "print('Num. env files:', len(env_files))\n",
    "print('Num. files:', len(fnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# clas_dict = dict()\n",
    "# for e in env_files:\n",
    "#     fname = e.split('/')[-1]\n",
    "#     underscore_idx = [m.start() for m in re.finditer('_', fname)]\n",
    "#     word_id = fname.find('word')\n",
    "#     clas = fname[word_id+4:underscore_idx[-1]]\n",
    "#     if clas not in clas_dict:\n",
    "#         clas_dict[clas] = 1\n",
    "#     else:\n",
    "#         clas_dict[clas] += 1\n",
    "# sorted_clas_dict = {k: v for k, v in sorted(clas_dict.items(), key=lambda item: item[1])}\n",
    "# reverse =dict(reversed(list(sorted_clas_dict.items())))\n",
    "# # select the ones with most samples\n",
    "# sorted_clas_dict = dict(itertools.islice(reverse.items(), num_class))\n",
    "# print(sorted_clas_dict)\n",
    "# select_classes = sorted_clas_dict.keys()\n",
    "# print('select_classes:', select_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classes = {'125': 0, '121': 1, '122': 2, '123': 3, '117': 4,\n",
    "               '119': 5, '131': 6, '128': 7, '129': 8, '118': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return np.array(dataframe.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrameCapture(path): \n",
    "      \n",
    "    # Path to video file \n",
    "    vidObj = cv2.VideoCapture(path) \n",
    "    video_length = int(vidObj.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     print (\"Number of frames: \", video_length)\n",
    "    # Used as counter variable \n",
    "    count = 0\n",
    "    success, frame = vidObj.read() \n",
    "    # checks whether frames were extracted \n",
    "#     success = 1\n",
    "    frames = []\n",
    "    while success: \n",
    "  \n",
    "        resized = cv2.resize(frame, (width,height), interpolation = cv2.INTER_LINEAR)\n",
    "        success, frame = vidObj.read() \n",
    "#         print('Read a new frame: ', success)\n",
    "        frames.append(resized) # .resized(width,height)\n",
    "        count += 1\n",
    "        if (count > (video_length-1)):\n",
    "            break\n",
    "    result = np.array([frames[i] for i in range(len(frames))])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrameCapture_gray(path): \n",
    "      \n",
    "    # Path to video file \n",
    "    vidObj = cv2.VideoCapture(path) \n",
    "    video_length = int(vidObj.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     print (\"Number of frames: \", video_length)\n",
    "    # Used as counter variable \n",
    "    count = 0\n",
    "    success, frame = vidObj.read() \n",
    "    # checks whether frames were extracted \n",
    "#     success = 1\n",
    "    frames = []\n",
    "    while success: \n",
    "  \n",
    "        resized = cv2.cvtColor(cv2.resize(frame, (width,height), interpolation = cv2.INTER_LINEAR), cv2.COLOR_BGR2GRAY)\n",
    "        success, frame = vidObj.read() \n",
    "#         print('Read a new frame: ', success)\n",
    "        frames.append(resized) # .resized(width,height)\n",
    "        count += 1\n",
    "        if (count > (video_length-1)):\n",
    "            break\n",
    "#     result = np.expand_dims(np.array([frames[i] for i in range(len(frames))]), -1)\n",
    "    result = np.array([frames[i] for i in range(len(frames))])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_video(vid, num_pad):\n",
    "#     print(vid.shape)\n",
    "#     print(vid[0].shape)\n",
    "#     print(vid[1].shape)\n",
    "    vid = np.concatenate([vid, np.tile(vid[-1,:,:], [num_pad, 1, 1, 1])])\n",
    "#     vid = np.concatenate([vid, np.tile(vid[-1,:,:], [num_pad, 1, 1])])\n",
    "    return vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022_10_13_12_52_55_subjLadi_Exp0_class125_part5_word125_cnt8',\n",
       " '2022_10_13_13_00_54_subjLadi_Exp0_class122_part1_word122_cnt9',\n",
       " '2022_10_13_13_04_32_subjLadi_Exp0_class123_part3_word123_cnt8',\n",
       " '2022_10_13_13_04_32_subjLadi_Exp0_class123_part4_word123_cnt9',\n",
       " '2022_10_13_13_13_51_subjLadi_Exp0_class119_part4_word119_cnt12']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(err_path, 'r') as err:\n",
    "    err_files = []\n",
    "    for line in err:\n",
    "        err_files.append(line.split(\",\")[0][:-1])\n",
    "err_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1924\n",
      "2022_10_13_16_12_38_subjEmre_Exp0_class128_part2_word128_cnt5\n",
      "2022_10_13_13_26_55_subjLadi_Exp0_class118_part4_word118_cnt10\n",
      "2022_10_13_15_32_17_subjSean_Exp0_class125_part1_word125_cnt11\n",
      "2022_10_13_16_19_32_subjEmre_Exp0_class118_part1_word118_cnt6\n",
      "2022_10_13_16_16_32_subjEmre_Exp0_class129_part5_word129_cnt3\n"
     ]
    }
   ],
   "source": [
    "# remove md files which doesn't have corresponding vid file due to labeling error\n",
    "for f in fnames:\n",
    "    if f in err_files:\n",
    "        fnames.remove(f)\n",
    "print(len(fnames))\n",
    "\n",
    "for f in fnames[:5]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, test_files = train_test_split(fnames, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training file: 1539/1539\n",
      "(1539, 150, 3, 42)\n",
      "(1539, 10)\n",
      "(1539, 50, 224, 224, 3)\n",
      "max_vid_len: 150\n"
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "train_vids = []\n",
    "train_skels = []\n",
    "train_labels = []\n",
    "max_vid_len = 150 # 5 sec x 30 frames\n",
    "train_mask = np.zeros((len(train_files), max_vid_len))\n",
    "\n",
    "for i, f in enumerate(train_files):\n",
    "    \n",
    "    underscore_idx = [m.start() for m in re.finditer('_', f)]\n",
    "    word_id = f.find('word')\n",
    "    clas = f[word_id+4:underscore_idx[-1]]\n",
    "    \n",
    "    if clas not in new_classes:\n",
    "        continue\n",
    "        \n",
    "    # read envelope\n",
    "#     env = load_file(e)\n",
    "#     if sum(env[0]) < env_threshold:\n",
    "#         continue\n",
    "        \n",
    "    # read video\n",
    "    vid_name = vid_path + f + '.avi'\n",
    "#     vid = FrameCapture_gray(vid_name)\n",
    "    vid = FrameCapture(vid_name)\n",
    "    num_frames = vid.shape[0]\n",
    "    if num_frames > max_vid_len:\n",
    "        vid = vid[:max_vid_len]\n",
    "    elif num_frames < max_vid_len:\n",
    "        num_pad = max_vid_len - num_frames\n",
    "        train_mask[i, -num_pad:] = 1\n",
    "        vid = pad_video(np.array(vid), num_pad)\n",
    "    train_vids.append(vid[::3])\n",
    "    \n",
    "    # read skeleton\n",
    "    skel_name = skel_path + f + '.mat'\n",
    "    skel = loadmat(skel_name)['hands']\n",
    "    skel = np.transpose(skel, [2, 1, 0])\n",
    "    skel = skel[3:]\n",
    "    \n",
    "    # pad missing frames\n",
    "    if skel.shape[0] < max_vid_len:\n",
    "        skel = np.concatenate([skel, np.tile(np.expand_dims(skel[0], 0), [max_vid_len - skel.shape[0], 1, 1])])\n",
    "    elif skel.shape[0] > max_vid_len:\n",
    "        skel = skel[:max_vid_len]\n",
    "    train_skels.append(skel)\n",
    "    \n",
    "    # read microDoppler\n",
    "#     md_name = md_path + f + '.png'\n",
    "#     img = cv2.imread(md_name)\n",
    "#     img = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC) # resize to (128,128)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # cv2 load images as BGR, convert it to RGB\n",
    "    clear_output(wait=True)\n",
    "    print('Processing training file: '+ str(i+1) + '/' + str(len(train_files)))\n",
    "#     train_images.append(img)\n",
    "    train_labels.append(new_classes[clas])\n",
    "#     if i == 2:\n",
    "#         break\n",
    "\n",
    "train_labels = to_categorical(np.array(train_labels), num_class)\n",
    "# train_images = np.swapaxes(np.array(train_images), 1, 2).reshape(len(train_images), width, height, channels)/255.\n",
    "# train_images = np.swapaxes(train_images, 1, 3)\n",
    "train_vids = np.array(train_vids)/255.\n",
    "train_skels = np.array(train_skels)\n",
    "# print(train_images.shape)\n",
    "print(train_skels.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_vids.shape)\n",
    "print('max_vid_len:', max_vid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i.shape for i in train_skels if len(i) != 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing testing file: 385/385\n",
      "(385, 150, 3, 42)\n",
      "(385, 10)\n",
      "(385, 50, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "test_images = []\n",
    "test_vids = []\n",
    "test_skels = []\n",
    "test_labels = []\n",
    "test_mask = np.zeros((len(test_files), max_vid_len))\n",
    "\n",
    "for i, f in enumerate(test_files):\n",
    "    \n",
    "    underscore_idx = [m.start() for m in re.finditer('_', f)]\n",
    "    word_id = f.find('word')\n",
    "    clas = f[word_id+4:underscore_idx[-1]]\n",
    "    \n",
    "    if clas not in new_classes:\n",
    "        continue\n",
    "    \n",
    "    # read envelope\n",
    "#     env = load_file(e)\n",
    "#     if sum(env[0]) < env_threshold:\n",
    "#         continue\n",
    "        \n",
    "    # read video\n",
    "    vid_name = vid_path + f + '.avi'\n",
    "    vid = FrameCapture(vid_name)\n",
    "#     vid = FrameCapture_gray(vid_name)\n",
    "    num_frames = vid.shape[0]\n",
    "    if num_frames > max_vid_len:\n",
    "        max_vid_len = num_frames\n",
    "        vid = vid[:max_vid_len]\n",
    "    elif num_frames < max_vid_len:\n",
    "        num_pad = max_vid_len - num_frames\n",
    "        test_mask[i, -num_pad:] = 1\n",
    "        vid = pad_video(np.array(vid), num_pad)\n",
    "    test_vids.append(vid[::3])\n",
    "    \n",
    "    # read skeleton\n",
    "    skel_name = skel_path + f + '.mat'\n",
    "    skel = loadmat(skel_name)['hands']\n",
    "    skel = np.transpose(skel, [2, 1, 0])\n",
    "    skel = skel[3:]\n",
    "    \n",
    "    # pad missing frames\n",
    "    if skel.shape[0] < max_vid_len:\n",
    "        skel = np.concatenate([skel, np.tile(np.expand_dims(skel[0], 0), [max_vid_len - skel.shape[0], 1, 1])])\n",
    "    elif skel.shape[0] > max_vid_len:\n",
    "        skel = skel[:max_vid_len]\n",
    "    test_skels.append(skel)\n",
    "    \n",
    "    # read microDoppler\n",
    "#     md_name = md_path + f + '.png'\n",
    "#     img = cv2.imread(md_name)\n",
    "#     img = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC) # resize to (128,128)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # cv2 load images as BGR, convert it to RGB\n",
    "    clear_output(wait=True)\n",
    "    print('Processing testing file: '+ str(i+1) + '/' + str(len(test_files)))\n",
    "#     test_images.append(img)\n",
    "    test_labels.append(new_classes[clas])\n",
    "\n",
    "test_labels = to_categorical(np.array(test_labels), num_class)\n",
    "# test_images = np.swapaxes(np.array(test_images), 1, 2).reshape(len(test_images), width, height, channels)/255.\n",
    "# test_images = np.swapaxes(test_images, 1, 3)\n",
    "test_vids = np.array(test_vids)/255.\n",
    "test_skels = np.array(test_skels)\n",
    "# print(test_images.shape)\n",
    "print(test_skels.shape)\n",
    "print(test_labels.shape)\n",
    "print(test_vids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in test_vids if len(i) != 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/HDD04/ICASSP_2023/Scripts/datasets/imit_ASL_10_vid_skel_224x224rgb_down.pkl created.\n"
     ]
    }
   ],
   "source": [
    "data = [train_files, train_vids, train_skels, train_labels, train_mask,\n",
    "        test_files, test_vids, test_skels, test_labels, test_mask]\n",
    "# data = [train_images, train_labels, train_files,\n",
    "#         test_images, test_labels, test_files]\n",
    "\n",
    "with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
    "print(filename+' created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
